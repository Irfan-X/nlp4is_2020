{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Unit 7\n",
    "\n",
    "## Part 1: WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See tutorial on Website -- Here we provide additional content / Exercise results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f34d004c790>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Computing Word Embeddings: Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n",
      "\n",
      "Number of training examples: 58\n",
      "vocab_size 49\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "def create_context_data(raw_text):\n",
    "\n",
    "    data = []\n",
    "    for i in range(2, len(raw_text) - 2):\n",
    "        context = [raw_text[i - 2], raw_text[i - 1],\n",
    "                   raw_text[i + 1], raw_text[i + 2]]\n",
    "        target = raw_text[i]\n",
    "        data.append((context, target))\n",
    "\n",
    "    print(data[:5])\n",
    "    print(\"\\nNumber of training examples:\", len(data))\n",
    "    print(\"vocab_size\", vocab_size)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = create_context_data(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 35, 46,  0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**initial skeleton code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size *  2 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[228.26647400856018, 225.08308219909668, 221.94002175331116, 218.83432626724243, 215.75911283493042, 212.70163893699646, 209.65577483177185, 206.61933946609497, 203.59706830978394, 200.58099508285522, 197.58383011817932, 194.58465337753296, 191.5872826576233, 188.6017553806305, 185.59955596923828, 182.58750987052917, 179.5686798095703, 176.5377436876297, 173.50407207012177, 170.46508395671844]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "                \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        #print(context, target)\n",
    "        context_idxs = make_context_vector(context, word_to_ix)  # example\n",
    "        #print(context_ids)\n",
    "        \n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "    \n",
    "### Do some predictions to see if works somehow :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'computational'], 'idea'), (['the', 'idea', 'computational', 'processes'], 'of')]\n",
      "\n",
      "Number of training examples: 3\n",
      "vocab_size 49\n",
      "[(['We', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'computational'], 'idea'), (['the', 'idea', 'computational', 'processes'], 'of')]\n",
      "['We', 'study', 'idea', 'of'] the\n",
      "['study', 'the', 'of', 'computational'] idea\n",
      "['the', 'idea', 'computational', 'processes'] of\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"We study the idea of computational processes\".split()\n",
    "test_data = create_context_data(test_sentence)\n",
    "print(test_data)\n",
    "\n",
    "## we need an idx-to-word\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "#print(ix_to_word)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for context, target in test_data:\n",
    "        context_idxs = make_context_vector(context, word_to_ix)  # example\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        # argmax give the index of the entry with the largest value\n",
    "        pred_idx = log_probs.argmax().item()\n",
    "        print(context, ix_to_word[pred_idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ok see it doesn't make too much sense yet, the training data is too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: LSTM: SEQUENCE MODELS AND LONG-SHORT TERM MEMORY NETWORKS\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[-0.8723, -1.0225, -1.5038],\n",
      "        [-1.0115, -0.8982, -1.4740],\n",
      "        [-1.0048, -0.9135, -1.4577],\n",
      "        [-1.0608, -0.8628, -1.4617],\n",
      "        [-1.0379, -0.8689, -1.4856]])\n",
      "\n",
      "\n",
      "tensor([[-0.0400, -3.4591, -4.8612],\n",
      "        [-4.4714, -0.0237, -4.4215],\n",
      "        [-3.9015, -3.3954, -0.0552],\n",
      "        [-0.0293, -3.8717, -4.8273],\n",
      "        [-3.8551, -0.0268, -5.2412]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    print(inputs)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(\"\\n\")\n",
    "    print(tag_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Extend with Char-Level features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The dog ate the apple', 'Everybody read that book']\n",
      "['The dog ate the appl', 'Everybody read that boo']\n",
      "TT ['he dog ate the apple', 'verybody read that book']\n",
      "{'T': 0, 'h': 1, 'e': 2, ' ': 3, 'd': 4, 'o': 5, 'g': 6, 'a': 7, 't': 8, 'p': 9, 'l': 10, 'E': 11, 'v': 12, 'r': 13, 'y': 14, 'b': 15, 'k': 16}\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 3 \n",
    "\n",
    "# 0.) create a list of words in the training texts\n",
    "\n",
    "sentences = [\" \".join(sentence) for sentence, _ in training_data]\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "# create the input and target sequences\n",
    "inputs  = [s[:-1] for s in sentences]\n",
    "targets = [s[1:] for s in sentences]\n",
    "\n",
    "print(inputs)\n",
    "print(\"Targets\", targets)\n",
    "\n",
    "# 1.) create char_to_ix\n",
    "char_to_ix = {}\n",
    "for sentence in sentences:\n",
    "    for c in sentence:\n",
    "        if c not in char_to_ix:\n",
    "            char_to_ix[c] = len(char_to_ix)\n",
    "\n",
    "print(char_to_ix)\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(char_to_ix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2char = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        char_space = self.hidden2char(lstm_out.view(len(sentence), -1))\n",
    "        char_scores = F.log_softmax(char_space, dim=1)\n",
    "        return char_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The dog ate the appl', 'Everybody read that boo']\n",
      "TTT ['he dog ate the apple', 'verybody read that book']\n",
      "{'T': 0, 'h': 1, 'e': 2, ' ': 3, 'd': 4, 'o': 5, 'g': 6, 'a': 7, 't': 8, 'p': 9, 'l': 10, 'E': 11, 'v': 12, 'r': 13, 'y': 14, 'b': 15, 'k': 16}\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  3,  7,  8,  2,  3,  8,  1,  2,  3,  7,  9,\n",
      "         9, 10])\n",
      "Loss: 2.8382\n",
      "Loss: 2.8191\n",
      "Loss: 2.5086\n",
      "Loss: 2.6656\n",
      "Loss: 2.3774\n",
      "Loss: 2.6231\n",
      "Loss: 2.2739\n",
      "Loss: 2.5900\n",
      "Loss: 2.1625\n",
      "Loss: 2.5534\n",
      "Loss: 2.0468\n",
      "Loss: 2.5058\n",
      "Loss: 1.9312\n",
      "Loss: 2.4368\n",
      "Loss: 1.8171\n",
      "Loss: 2.3421\n",
      "Loss: 1.7058\n",
      "Loss: 2.2243\n",
      "Loss: 1.5912\n",
      "Loss: 2.1121\n",
      "Loss: 1.4786\n",
      "Loss: 1.9888\n",
      "Loss: 1.3745\n",
      "Loss: 1.8660\n",
      "Loss: 1.2780\n",
      "Loss: 1.7283\n",
      "Loss: 1.1876\n",
      "Loss: 1.5861\n",
      "Loss: 1.1058\n",
      "Loss: 1.4577\n",
      "Loss: 1.0688\n",
      "Loss: 1.3474\n",
      "Loss: 0.9882\n",
      "Loss: 1.2550\n",
      "Loss: 0.9232\n",
      "Loss: 1.1666\n",
      "Loss: 0.8634\n",
      "Loss: 1.0823\n",
      "Loss: 0.8153\n",
      "Loss: 1.0111\n",
      "Loss: 0.7570\n",
      "Loss: 0.9469\n",
      "Loss: 0.7006\n",
      "Loss: 0.8691\n",
      "Loss: 0.6645\n",
      "Loss: 0.8182\n",
      "Loss: 0.5999\n",
      "Loss: 0.7626\n",
      "Loss: 0.7554\n",
      "Loss: 1.1590\n",
      "Loss: 0.5251\n",
      "Loss: 0.6717\n",
      "Loss: 0.4832\n",
      "Loss: 0.6253\n",
      "Loss: 0.4520\n",
      "Loss: 0.5867\n",
      "Loss: 0.4218\n",
      "Loss: 0.5498\n",
      "Loss: 0.3937\n",
      "Loss: 0.5158\n",
      "Loss: 0.3676\n",
      "Loss: 0.4843\n",
      "Loss: 0.3434\n",
      "Loss: 0.4552\n",
      "Loss: 0.3204\n",
      "Loss: 0.4284\n",
      "Loss: 0.2924\n",
      "Loss: 0.4013\n",
      "Loss: 0.4153\n",
      "Loss: 0.6290\n",
      "Loss: 0.2603\n",
      "Loss: 0.3589\n",
      "Loss: 0.2462\n",
      "Loss: 0.3414\n",
      "Loss: 0.2358\n",
      "Loss: 0.3470\n",
      "Loss: 0.7013\n",
      "Loss: 0.8682\n",
      "Loss: 0.4548\n",
      "Loss: 0.5019\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  3,  7,  8,  2,  3,  8,  1,  2,  3,  7,  9,\n",
      "         9, 10])\n",
      "\n",
      "\n",
      "tensor([[-7.4438e+00, -7.3716e-01, -1.2152e+00, -6.4126e+00, -5.2044e+00,\n",
      "         -1.9368e+00, -3.0482e+00, -4.4317e+00, -6.4745e+00, -5.8520e+00,\n",
      "         -7.4961e+00, -7.4767e+00, -5.2272e+00, -8.6973e+00, -6.9849e+00,\n",
      "         -8.6854e+00, -6.5108e+00],\n",
      "        [-9.0105e+00, -4.3475e+00, -5.4583e-01, -8.2480e+00, -6.2382e+00,\n",
      "         -9.9137e-01, -4.0227e+00, -9.5098e+00, -1.0357e+01, -6.8909e+00,\n",
      "         -6.9912e+00, -9.2728e+00, -6.4027e+00, -9.7878e+00, -5.7506e+00,\n",
      "         -1.0963e+01, -4.6732e+00],\n",
      "        [-1.0689e+01, -9.4974e+00, -7.4503e+00, -3.3697e-02, -5.3973e+00,\n",
      "         -9.4610e+00, -4.6711e+00, -1.0894e+01, -1.1127e+01, -1.0987e+01,\n",
      "         -6.3833e+00, -1.0742e+01, -9.1619e+00, -9.0728e+00, -8.1038e+00,\n",
      "         -8.5869e+00, -4.1341e+00],\n",
      "        [-8.2791e+00, -9.3051e+00, -5.3962e+00, -4.5416e+00, -4.4603e-01,\n",
      "         -6.5215e+00, -4.8627e+00, -3.5401e+00, -4.0985e+00, -8.7080e+00,\n",
      "         -7.2373e+00, -7.9702e+00, -8.3419e+00, -2.6256e+00, -2.6396e+00,\n",
      "         -1.9627e+00, -5.5828e+00],\n",
      "        [-8.7698e+00, -2.0899e+00, -2.7384e+00, -1.1380e+01, -4.0750e+00,\n",
      "         -3.2087e-01, -3.0946e+00, -3.9576e+00, -6.3019e+00, -6.8237e+00,\n",
      "         -1.0794e+01, -8.4396e+00, -7.3011e+00, -9.6433e+00, -7.7792e+00,\n",
      "         -9.5537e+00, -8.7472e+00],\n",
      "        [-8.1843e+00, -1.6979e+00, -4.7328e+00, -4.2299e+00, -2.6184e+00,\n",
      "         -3.2977e+00, -5.1002e-01, -3.1024e+00, -3.7858e+00, -4.5389e+00,\n",
      "         -6.9538e+00, -7.4132e+00, -7.1610e+00, -1.1575e+01, -1.0231e+01,\n",
      "         -7.9091e+00, -6.2888e+00],\n",
      "        [-1.0939e+01, -5.1872e+00, -8.5881e+00, -3.8602e-02, -7.9306e+00,\n",
      "         -1.4070e+01, -6.0932e+00, -3.8258e+00, -5.1199e+00, -8.3242e+00,\n",
      "         -7.1551e+00, -1.0265e+01, -1.0402e+01, -1.3087e+01, -1.3051e+01,\n",
      "         -7.5682e+00, -9.5735e+00],\n",
      "        [-1.0485e+01, -7.5818e+00, -7.7544e+00, -6.3318e+00, -4.3836e+00,\n",
      "         -1.1429e+01, -7.3074e+00, -2.2080e-01, -1.8577e+00, -8.5062e+00,\n",
      "         -9.6252e+00, -9.5738e+00, -1.1336e+01, -8.2475e+00, -8.3803e+00,\n",
      "         -3.6759e+00, -1.1302e+01],\n",
      "        [-7.8753e+00, -4.9471e+00, -4.9430e+00, -8.8105e+00, -2.1405e+00,\n",
      "         -3.1539e+00, -3.8096e+00, -4.9014e+00, -7.8030e-01, -1.1242e+00,\n",
      "         -5.2910e+00, -7.2934e+00, -6.1283e+00, -1.0444e+01, -9.6161e+00,\n",
      "         -5.8911e+00, -6.5573e+00],\n",
      "        [-6.8785e+00, -1.7612e+00, -6.5804e-01, -2.3454e+00, -7.9624e+00,\n",
      "         -8.4531e+00, -7.0132e+00, -3.2395e+00, -2.9492e+00, -3.1426e+00,\n",
      "         -2.8666e+00, -7.0671e+00, -4.4967e+00, -8.1811e+00, -6.7949e+00,\n",
      "         -5.1990e+00, -6.5385e+00],\n",
      "        [-1.1091e+01, -1.0024e+01, -1.0225e+01, -2.8852e-02, -8.0937e+00,\n",
      "         -1.7583e+01, -1.0011e+01, -6.3096e+00, -4.3520e+00, -8.3541e+00,\n",
      "         -5.4596e+00, -1.0816e+01, -1.0138e+01, -1.0101e+01, -1.1571e+01,\n",
      "         -4.7569e+00, -9.2004e+00],\n",
      "        [-8.7215e+00, -8.1982e+00, -5.4534e+00, -7.3197e+00, -3.8983e+00,\n",
      "         -9.9861e+00, -8.8590e+00, -1.2971e+00, -7.4288e-01, -6.4089e+00,\n",
      "         -7.2893e+00, -8.3470e+00, -8.5492e+00, -4.7518e+00, -5.2503e+00,\n",
      "         -1.5684e+00, -9.6211e+00],\n",
      "        [-9.3588e+00, -2.8530e-01, -1.5063e+00, -6.9545e+00, -1.0440e+01,\n",
      "         -6.7661e+00, -6.5049e+00, -4.4390e+00, -6.1718e+00, -4.9673e+00,\n",
      "         -7.1137e+00, -9.2529e+00, -7.0137e+00, -1.3725e+01, -1.0805e+01,\n",
      "         -1.0779e+01, -9.6098e+00],\n",
      "        [-8.0987e+00, -2.3506e+00, -4.4586e-01, -3.3707e+00, -6.7931e+00,\n",
      "         -7.2254e+00, -5.2957e+00, -1.6846e+00, -4.0841e+00, -5.2886e+00,\n",
      "         -5.0107e+00, -7.8044e+00, -7.2218e+00, -8.8298e+00, -5.6180e+00,\n",
      "         -5.8478e+00, -6.8909e+00],\n",
      "        [-1.2956e+01, -9.8427e+00, -1.1486e+01, -4.7684e-03, -1.0173e+01,\n",
      "         -1.9531e+01, -1.0341e+01, -6.1544e+00, -6.6483e+00, -1.1208e+01,\n",
      "         -7.9544e+00, -1.2482e+01, -1.2582e+01, -1.2624e+01, -1.3560e+01,\n",
      "         -7.1168e+00, -1.1292e+01],\n",
      "        [-9.1668e+00, -7.6310e+00, -5.8259e+00, -6.7441e+00, -4.2107e+00,\n",
      "         -1.0465e+01, -8.3360e+00, -6.0274e-01, -1.1641e+00, -7.2180e+00,\n",
      "         -8.0394e+00, -8.6417e+00, -9.3494e+00, -5.6563e+00, -5.9639e+00,\n",
      "         -2.1782e+00, -1.0175e+01],\n",
      "        [-8.8747e+00, -7.1946e+00, -3.3601e+00, -1.1815e+01, -3.7165e+00,\n",
      "         -2.9334e+00, -5.9353e+00, -7.1273e+00, -1.7174e+00, -3.7362e-01,\n",
      "         -4.4877e+00, -8.3962e+00, -6.9529e+00, -1.1416e+01, -8.4089e+00,\n",
      "         -6.7392e+00, -6.3382e+00],\n",
      "        [-9.1245e+00, -7.7966e+00, -3.5856e+00, -1.0078e+01, -5.8192e+00,\n",
      "         -4.8184e+00, -7.2874e+00, -9.9493e+00, -3.1930e+00, -1.5596e-01,\n",
      "         -2.8668e+00, -8.9893e+00, -6.2410e+00, -1.2422e+01, -9.5196e+00,\n",
      "         -7.7847e+00, -5.5606e+00],\n",
      "        [-8.4056e+00, -9.2409e+00, -3.1494e+00, -2.8450e+00, -5.8778e+00,\n",
      "         -9.8712e+00, -8.2340e+00, -8.0141e+00, -3.0107e+00, -2.3470e+00,\n",
      "         -3.5750e-01, -8.4384e+00, -6.4196e+00, -8.2247e+00, -5.8653e+00,\n",
      "         -3.7560e+00, -3.7914e+00],\n",
      "        [-7.8058e+00, -7.1499e+00, -1.5124e-01, -7.8669e+00, -6.3959e+00,\n",
      "         -3.5290e+00, -7.6693e+00, -1.0557e+01, -7.6773e+00, -4.5145e+00,\n",
      "         -3.5774e+00, -8.5317e+00, -4.4366e+00, -6.3373e+00, -3.5452e+00,\n",
      "         -7.1874e+00, -3.7089e+00]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CharLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(char_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(inputs)\n",
    "print(\"TTT\", targets)\n",
    "\n",
    "print(char_to_ix)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs_in = prepare_sequence(inputs[0], char_to_ix)\n",
    "    print(inputs_in)\n",
    "    char_scores = model(inputs_in)\n",
    "\n",
    "                 \n",
    "\n",
    "for epoch in range(2000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for i in range(len(inputs)):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sent_in = prepare_sequence(inputs[i], char_to_ix)\n",
    "        targets_in = prepare_sequence(targets[i], char_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        char_scores = model(sent_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(char_scores, targets_in)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if epoch%50 == 0:\n",
    "        #print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "# See what the scores are after training\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs_in = prepare_sequence(inputs[0], char_to_ix)\n",
    "    print(inputs_in)\n",
    "    char_scores = model(inputs_in)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(char_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 3])\n",
      "\n",
      "\n",
      "tensor([[ -7.4438,  -0.7372,  -1.2152,  -6.4126,  -5.2044,  -1.9368,  -3.0482,\n",
      "          -4.4317,  -6.4745,  -5.8520,  -7.4961,  -7.4767,  -5.2272,  -8.6973,\n",
      "          -6.9849,  -8.6854,  -6.5108],\n",
      "        [ -9.0105,  -4.3475,  -0.5458,  -8.2480,  -6.2382,  -0.9914,  -4.0227,\n",
      "          -9.5098, -10.3575,  -6.8909,  -6.9912,  -9.2728,  -6.4027,  -9.7878,\n",
      "          -5.7506, -10.9634,  -4.6732],\n",
      "        [-10.6887,  -9.4974,  -7.4503,  -0.0337,  -5.3973,  -9.4610,  -4.6711,\n",
      "         -10.8939, -11.1271, -10.9869,  -6.3833, -10.7422,  -9.1619,  -9.0728,\n",
      "          -8.1038,  -8.5869,  -4.1341],\n",
      "        [ -8.2791,  -9.3051,  -5.3962,  -4.5416,  -0.4460,  -6.5215,  -4.8627,\n",
      "          -3.5401,  -4.0985,  -8.7080,  -7.2373,  -7.9702,  -8.3419,  -2.6256,\n",
      "          -2.6396,  -1.9627,  -5.5828],\n",
      "        [ -8.7698,  -2.0899,  -2.7384, -11.3802,  -4.0750,  -0.3209,  -3.0946,\n",
      "          -3.9576,  -6.3019,  -6.8237, -10.7937,  -8.4396,  -7.3011,  -9.6433,\n",
      "          -7.7792,  -9.5537,  -8.7472],\n",
      "        [ -8.1843,  -1.6979,  -4.7328,  -4.2299,  -2.6184,  -3.2977,  -0.5100,\n",
      "          -3.1024,  -3.7858,  -4.5389,  -6.9538,  -7.4132,  -7.1610, -11.5754,\n",
      "         -10.2307,  -7.9091,  -6.2888],\n",
      "        [-10.9394,  -5.1872,  -8.5881,  -0.0386,  -7.9306, -14.0699,  -6.0932,\n",
      "          -3.8258,  -5.1199,  -8.3242,  -7.1551, -10.2653, -10.4017, -13.0870,\n",
      "         -13.0509,  -7.5682,  -9.5735],\n",
      "        [-10.4851,  -7.5818,  -7.7544,  -6.3318,  -4.3836, -11.4287,  -7.3074,\n",
      "          -0.2208,  -1.8577,  -8.5062,  -9.6252,  -9.5738, -11.3362,  -8.2475,\n",
      "          -8.3803,  -3.6759, -11.3018]])\n"
     ]
    }
   ],
   "source": [
    "### test\n",
    "test_sent = \"The dog \"\n",
    "\n",
    "print(char_to_ix['e'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs_in = prepare_sequence(test_sent, char_to_ix)\n",
    "    print(inputs_in)\n",
    "    scores = model(inputs_in)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
