{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Unit 4\n",
    "\n",
    "Unit 4 has the following parts\n",
    "* Finish Word Embeddings from unit 3: start at slide 30\n",
    "* Look at slides about important basic NLP tasks, such as POS-tagging, NER, parsing etc using these slides: https://www.slideshare.net/GirishKhanzode/nlp-52218202\n",
    "* In combination with the slides we will also look at spaCy to practice things\n",
    "* **this unit will be organized from this ipynb notebook, which we use back and forth with slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "* spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "* NLTK is guided more towards education, and is quite old.\n",
    "* spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "\n",
    "## Resources:\n",
    "### Resource 1: Web site:  https://spacy.io/\n",
    "### Resource 2: Basic usage and functions: https://spacy.io/usage/spacy-101\n",
    "### Resource 3: Course:  very nice course found here: https://course.spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/wohlg/.local/lib/python3.7/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages/numpy-1.17.0-py3.7-linux-x86_64.egg (from spacy) (1.17.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (41.1.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/wohlg/.local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/wohlg/.local/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.33.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/wohlg/.local/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.0.0)\n",
      "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /home/wohlg/.local/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/wohlg/.local/lib/python3.7/site-packages (from en_core_web_md==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (41.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages/numpy-1.17.0-py3.7-linux-x86_64.egg (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.17.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/wohlg/.local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/wohlg/.local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/wohlg/.local/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_md==2.2.5) (4.33.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/wohlg/.local/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# if not installed yet\n",
    "!pip3 install spacy\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "\n",
    "### Slides: Basic NLP (repetition) 2-10\n",
    "While the installation is running, let's look at the slides: 2-10</font>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 10\n",
      "This is a sentence, which has two parts.\n",
      "en\n",
      "[This is a sentence, which has two parts.]\n"
     ]
    }
   ],
   "source": [
    "# analyze a piece of text with the model\n",
    "doc = nlp(u'This is a sentence, which has two parts.')\n",
    "print(doc.text)\n",
    "print(doc.lang_)\n",
    "print(list(doc.sents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's just look at the spaCy course: https://course.spacy.io/ Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "### First small exercise:\n",
    "* create an example sentence\n",
    "* create a spaCy object\n",
    "* iterate of tokens, show part-of-speech tags, if the token is alpha-numeric, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy simple example (dependence parse, pos, ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj [looking] Apple\n",
      "is AUX aux [looking] be\n",
      "looking VERB ROOT [] look\n",
      "at ADP prep [looking] at\n",
      "buying VERB pcomp [at, looking] buy\n",
      "U.K. PROPN compound [startup, buying, at, looking] U.K.\n",
      "startup NOUN dobj [buying, at, looking] startup\n",
      "for ADP prep [buying, at, looking] for\n",
      "$ SYM quantmod [billion, for, buying, at, looking] $\n",
      "1 NUM compound [billion, for, buying, at, looking] 1\n",
      "billion NUM pobj [for, buying, at, looking] billion\n"
     ]
    }
   ],
   "source": [
    "docX = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in docX:\n",
    "    print(token.text, token.pos_, token.dep_, list(token.ancestors), token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy can be used with many languages. In the simple way shown below we can use it without statistical ML  models. How to use a better model for Russian, see below.\n",
    "\n",
    "<font color='red'> maybe these simple models only do tokenization and other simple rule-based operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет Привет PROPN True False\n",
      "Миру Миру PROPN True False\n",
      "! ! PUNCT False True\n",
      "Как Как PROPN True False\n",
      "твои твои PROPN True False\n",
      "дела дела PROPN True False\n",
      "? ? PUNCT False True\n",
      "Сегодня Сегодня PROPN True False\n",
      "неплохая неплохая PROPN True False\n",
      "погода погода PROPN True False\n",
      ". . PUNCT False True\n",
      "\n",
      "Guten Guten PROPN True False\n",
      "Tag Tag PROPN True False\n",
      ", , PUNCT False True\n",
      "wie wie PROPN True False\n",
      "heissen heissen PROPN True False\n",
      "Sie Sie PROPN True False\n",
      "denn denn PROPN True False\n",
      "? ? PUNCT False True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.ru import Russian\n",
    "doc = nlp(\"Привет Миру! Как твои дела? Сегодня неплохая погода.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_alpha, token.is_punct)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "from spacy.lang.de import German\n",
    "doc = nlp(\"Guten Tag, wie heissen Sie denn?\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_alpha, token.is_punct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "# Let's look at the next part of the slides: 11-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>\n",
    "    \n",
    "# Start of Unit 5\n",
    "\n",
    "<font color='red'>\n",
    "    \n",
    "### Quick repetition?\n",
    "* What is spaCy?\n",
    "* What did we look at last time with spaCy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some spaCy features in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple\n",
      "is be\n",
      "looking look\n",
      "at at\n",
      "buying buy\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n"
     ]
    }
   ],
   "source": [
    "# lemmatization example\n",
    "docX = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in docX:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech, Stopwords, is_alpha, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP Xxxxx True False\n",
      "is be AUX VBZ xx True True\n",
      "looking look VERB VBG xxxx True False\n",
      "at at ADP IN xx True True\n",
      "buying buy VERB VBG xxxx True False\n",
      "U.K. U.K. PROPN NNP X.X. False False\n",
      "startup startup NOUN NN xxxx True False\n",
      "for for ADP IN xxx True True\n",
      "$ $ SYM $ $ False False\n",
      "1 1 NUM CD d False False\n",
      "billion billion NUM CD xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper noun\n",
      "cardinal number\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"PROPN\"))\n",
    "print(spacy.explain(\"CD\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunking and parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "# Let's look at the dependency parsing slideset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple chunking:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Apple, U.K. startup]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Simple chunking:\")\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj looking\n",
      "is AUX aux looking\n",
      "looking VERB ROOT looking\n",
      "at ADP prep looking\n",
      "buying VERB pcomp at\n",
      "U.K. PROPN compound startup\n",
      "startup NOUN dobj buying\n",
      "for ADP prep buying\n",
      "$ SYM quantmod billion\n",
      "1 NUM compound billion\n",
      "billion NUM pobj for\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nominal subject\n",
      "auxiliary\n",
      "prepositional modifier\n",
      "complement of preposition\n",
      "object of preposition\n",
      "noun phrase as adverbial modifier\n",
      "case marking\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"nsubj\"))\n",
    "print(spacy.explain(\"aux\"))\n",
    "print(spacy.explain(\"prep\"))\n",
    "print(spacy.explain(\"pcomp\"))\n",
    "print(spacy.explain(\"pobj\"))\n",
    "print(spacy.explain(\"npadvmod\"))\n",
    "print(spacy.explain(\"case\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe PROPN nsubj bought\n",
      "bought VERB ROOT bought\n",
      "an DET det apple\n",
      "apple NOUN dobj bought\n",
      "in ADP prep bought\n",
      "France PROPN pobj in\n",
      "this DET det morning\n",
      "morning NOUN npadvmod bought\n",
      ". PUNCT punct bought\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Joe bought an apple in France this morning.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "### Small exercise on dependency parsing (Goal: understand and work with *basic sentence structure*)\n",
    "\n",
    "Write a script that\n",
    "* has a sentence as input\n",
    "* it gives you the basic \"subject\", \"predicate (root)\" and \"object\" (direct object) of the sentence\n",
    "* Output is: Subject: XX, Predicate: XX, Object:XX\n",
    "* print some more information about S-P-O, print their lemma, if the subject and object have a determiner, if there is a npadvmod for the predicate (root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe nsubj bought\n",
      "bought ROOT bought\n",
      "an det apple\n",
      "apple dobj bought\n",
      "in prep bought\n",
      "France pobj in\n",
      "this det morning\n",
      "morning npadvmod bought\n",
      ". punct bought\n",
      "XX bought bought\n",
      "XX bought bought\n",
      "XX bought apple\n",
      "XX bought bought\n",
      "XX bought bought\n",
      "XX bought in\n",
      "XX bought morning\n",
      "XX bought bought\n",
      "XX bought bought\n",
      "Joe bought apple\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Joe bought an apple in France this morning.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text)\n",
    "    if (token.dep_ == \"ROOT\"): \n",
    "        root = token.head.text\n",
    "    \n",
    "for token in doc:\n",
    "    print(\"XX\", root, token.head.text)\n",
    "    if token.head.text == root and token.dep_ == 'nsubj':\n",
    "        subj = token.text\n",
    "    if token.head.text == root and token.dep_ == 'dobj':\n",
    "        obj = token.text   \n",
    "\n",
    "print(subj,root,obj)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "# Let's look at the next part of the slides: \n",
    "* Semantics: 20-21\n",
    "* Pragmatics: 24-25\n",
    "* Challenges: 26-30\n",
    "* POS: 47-49\n",
    "* NER: 50-51\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 11\n",
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n",
      "\n",
      "\n",
      "Apple ORG\n",
      "is \n",
      "looking \n",
      "at \n",
      "buying \n",
      "U.K. GPE\n",
      "startup \n",
      "for \n",
      "$ MONEY\n",
      "1 MONEY\n",
      "billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some internals, OOV?, the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dog True 7.0336733 False\n",
      "Length of vector: 300 [-0.40176   0.37057   0.021281 -0.34125   0.049538  0.2944   -0.17376\n",
      " -0.27982   0.067622  2.1693  ]\n",
      "\n",
      " cat True 6.6808186 False\n",
      "Length of vector: 300 [-0.15067  -0.024468 -0.23368  -0.23378  -0.18382   0.32711  -0.22084\n",
      " -0.28777   0.12759   1.1656  ]\n",
      "\n",
      " banana True 6.700014 False\n",
      "Length of vector: 300 [ 0.20228  -0.076618  0.37032   0.032845 -0.41957   0.072069 -0.37476\n",
      "  0.05746  -0.012401  0.52949 ]\n",
      "\n",
      " afskfsd False 0.0 True\n",
      "Length of vector: 300 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(\"\\n\", token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "    print(\"Length of vector:\", len(token.vector), token.vector[:10]) # show only first part of vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "### Exercise 3: using *basic named entities and dependencies*\n",
    "* Take the text from a random Wikipedia Artikel (copy and paste)\n",
    "* Split the article into sentences with NLTK\n",
    "* Print all persons named in the article\n",
    "* Print all locations named in the article\n",
    "* Print person names that are subjects (nsubj) according to the dependency parser of the sentences.\n",
    "* Let's find out what the sentences talk about, which include both a location and a person. What are the\n",
    "    ROOTs (according to the dependency parser) of those sentences?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of War and Peace, by Leo Tolstoy\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org\r\n",
      "\r\n",
      "\r\n",
      "Title: War and Peace\r\n",
      "\r\n",
      "Author: Leo Tolstoy\r\n",
      "\r\n",
      "Posting Date: January 10, 2009 [EBook #2600]\r\n",
      "Release Date: April, 2001\r\n",
      "[Last updated: August 22, 2012]\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK WAR AND PEACE ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "An Anonymous Volunteer\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "WAR AND PEACE\r\n",
      "\r\n",
      "By Leo Tolstoy/Tolstoi\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "BOOK ONE: 1805\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "CHAPTER I\r\n",
      "\r\n",
      "\r\n",
      "\"Well, Prince, so Genoa and Lucca are now just family estates of the\r\n",
      "Buonapartes. But I warn you, if you don't tell me that this means war,\r\n",
      "if you still try to defend the infamies and horrors perpetrated by that\r\n",
      "Antichrist--I really believe he is Antichrist--I will have nothing more\r\n",
      "to do with you an\n"
     ]
    }
   ],
   "source": [
    "## download a random book -- take first 200 sentences\n",
    "import urllib  # the lib that handles the url stuff\n",
    "url = \"https://raw.githubusercontent.com/NSkelsey/cvf/master/war_and_peace.txt\"\n",
    "data = urllib.request.urlopen(url) # it's a file like object and works just like a file\n",
    "text = [line.decode('utf-8') for line in data]\n",
    "text = \"\".join(text[:200]) # first 100K characters of the book\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook ORG\n",
      "War and Peace WORK_OF_ART\n",
      "Leo Tolstoy PERSON\n",
      "the Project ORG\n",
      "eBook ORG\n",
      "War and Peace\r\n",
      "\r\n",
      " WORK_OF_ART\n",
      "Leo Tolstoy PERSON\n",
      "January 10, 2009 DATE\n",
      "2600 MONEY\n",
      "April, 2001 DATE\n",
      "August 22, 2012 DATE\n",
      "English LANGUAGE\n",
      "Leo Tolstoy PERSON\n",
      "Tolstoi\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      " PERSON\n",
      "ONE CARDINAL\n",
      "1805\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      " DATE\n",
      "Genoa GPE\n",
      "Lucca GPE\n",
      "the\r\n",
      "Buonapartes PERSON\n",
      "Antichrist PERSON\n",
      "July, 1805 DATE\n",
      "Anna Pavlovna\r\n",
      "Scherer PERSON\n",
      "Marya Fedorovna PERSON\n",
      "Prince Vasili Kuragin PERSON\n",
      "first ORDINAL\n",
      "Anna Pavlovna\r\n",
      " PERSON\n",
      "some days DATE\n",
      "la\r\n",
      " ORG\n",
      "St. Petersburg GPE\n",
      "French LANGUAGE\n",
      "an evening TIME\n",
      "tonight TIME\n",
      "7 CARDINAL\n",
      "10 CARDINAL\n",
      "Heavens WORK_OF_ART\n",
      "French NORP\n",
      "Anna Pavlovna PERSON\n",
      "First ORDINAL\n",
      "one CARDINAL\n",
      "one CARDINAL\n",
      "Anna Pavlovna PERSON\n",
      "English NORP\n",
      "Wednesday DATE\n",
      "today DATE\n",
      "Novosiltsev ORG\n",
      "Buonaparte PERSON\n",
      "Anna Pavlovna Scherer PERSON\n",
      "forty years DATE\n",
      "Anna Pavlovna PERSON\n",
      "Austria GPE\n",
      "Austria GPE\n",
      "Russia GPE\n",
      "Europe LOC\n",
      "one CARDINAL\n",
      "earth LOC\n",
      "England GPE\n",
      "Alexander PERSON\n",
      "Malta GPE\n",
      "English LANGUAGE\n",
      "Prussia GPE\n",
      "Buonaparte PERSON\n",
      "Europe LOC\n",
      "Hardenburg PERSON\n",
      "Haugwitz NORP\n",
      "Prussian NORP\n",
      "Europe LOC\n",
      "Wintzingerode ORG\n",
      "the King of\r\n",
      "Prussia's PERSON\n",
      "two CARDINAL\n",
      "tonight TIME\n",
      "le Vicomte de Mortemart FAC\n",
      "Montmorencys ORG\n",
      "Rohans NORP\n",
      "one CARDINAL\n",
      "French NORP\n",
      "the Abbe Morio ORG\n",
      "Baron Funke PERSON\n",
      "first ORDINAL\n",
      "Vienna GPE\n",
      "Marya Fedorovna PERSON\n",
      "Anna Pavlovna PERSON\n",
      "Funke PERSON\n",
      "Anna Pavlovna's PERSON\n",
      "Funke\r\n",
      " PERSON\n",
      "beaucoup d'estime PERSON\n",
      "Anna Pavlovna PERSON\n",
      "two CARDINAL\n",
      "Anatole PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize entities and dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize entities\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "doc_ent = nlp(u'When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.')\n",
    "\n",
    "displacy.render([doc_ent], style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"698820606a474f21b7cd3850a2727787-0\" class=\"displacy\" width=\"3900\" height=\"662.0\" direction=\"ltr\" style=\"max-width: none; height: 662.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">When</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Sebastian</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Thrun</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">started</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">working</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">self-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">driving</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">cars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">Google</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">2007,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">few</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">people</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">outside</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">company</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">took</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">him</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"572.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">seriously.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-0\" stroke-width=\"2px\" d=\"M70,527.0 C70,264.5 560.0,264.5 560.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,529.0 L62,517.0 78,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-1\" stroke-width=\"2px\" d=\"M245,527.0 C245,439.5 375.0,439.5 375.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,529.0 L237,517.0 253,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-2\" stroke-width=\"2px\" d=\"M420,527.0 C420,439.5 550.0,439.5 550.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,529.0 L412,517.0 428,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-3\" stroke-width=\"2px\" d=\"M595,527.0 C595,2.0 3375.0,2.0 3375.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,529.0 L587,517.0 603,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-4\" stroke-width=\"2px\" d=\"M595,527.0 C595,439.5 725.0,439.5 725.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M725.0,529.0 L733.0,517.0 717.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-5\" stroke-width=\"2px\" d=\"M770,527.0 C770,439.5 900.0,439.5 900.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M900.0,529.0 L908.0,517.0 892.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-6\" stroke-width=\"2px\" d=\"M1120,527.0 C1120,439.5 1250.0,439.5 1250.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,529.0 L1112,517.0 1128,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-7\" stroke-width=\"2px\" d=\"M1295,527.0 C1295,439.5 1425.0,439.5 1425.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,529.0 L1287,517.0 1303,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-8\" stroke-width=\"2px\" d=\"M945,527.0 C945,264.5 1435.0,264.5 1435.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,529.0 L1443.0,517.0 1427.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-9\" stroke-width=\"2px\" d=\"M770,527.0 C770,177.0 1615.0,177.0 1615.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,529.0 L1623.0,517.0 1607.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-10\" stroke-width=\"2px\" d=\"M1645,527.0 C1645,439.5 1775.0,439.5 1775.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1775.0,529.0 L1783.0,517.0 1767.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-11\" stroke-width=\"2px\" d=\"M770,527.0 C770,89.5 1970.0,89.5 1970.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1970.0,529.0 L1978.0,517.0 1962.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-12\" stroke-width=\"2px\" d=\"M1995,527.0 C1995,439.5 2125.0,439.5 2125.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2125.0,529.0 L2133.0,517.0 2117.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-13\" stroke-width=\"2px\" d=\"M2345,527.0 C2345,439.5 2475.0,439.5 2475.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,529.0 L2337,517.0 2353,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-14\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,177.0 3365.0,177.0 3365.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,529.0 L2512,517.0 2528,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-15\" stroke-width=\"2px\" d=\"M2520,527.0 C2520,439.5 2650.0,439.5 2650.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2650.0,529.0 L2658.0,517.0 2642.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-16\" stroke-width=\"2px\" d=\"M2695,527.0 C2695,439.5 2825.0,439.5 2825.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2825.0,529.0 L2833.0,517.0 2817.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-17\" stroke-width=\"2px\" d=\"M3045,527.0 C3045,439.5 3175.0,439.5 3175.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,529.0 L3037,517.0 3053,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-18\" stroke-width=\"2px\" d=\"M2870,527.0 C2870,352.0 3180.0,352.0 3180.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3180.0,529.0 L3188.0,517.0 3172.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-19\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,439.5 3525.0,439.5 3525.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3525.0,529.0 L3533.0,517.0 3517.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-698820606a474f21b7cd3850a2727787-0-20\" stroke-width=\"2px\" d=\"M3395,527.0 C3395,352.0 3705.0,352.0 3705.0,527.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-698820606a474f21b7cd3850a2727787-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3705.0,529.0 L3713.0,517.0 3697.0,517.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize entities\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "doc_ent = nlp(u'When Sebastian Thrun started working on self-driving cars at Google '\n",
    "u'in 2007, few people outside of the company took him seriously.')\n",
    "displacy.render([doc_ent], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token, Span and Doc similarity\n",
    "\n",
    "\n",
    "### Basic idea:\n",
    "\n",
    "* As we know, tokens have vectors (embeddings) associated with them (if using statistical models)\n",
    "* And: All the main spaCy structures like Token, Span, and Doc all have vectors assigned \n",
    "* Doc and Span vectors simply default to average of token vectors\n",
    "* This **simple** method of average vectors is not good for long documents, why?\n",
    "* With this information we can simply compute (using cosine similarity) the similarity between such objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Similarity between 2 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7791662980402656"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Example 1:  Determine semantic similarities\n",
    "doc1 = nlp(u'the fries were gross')\n",
    "doc2 = nlp(u'worst fries ever')\n",
    "doc1.similarity(doc2)\n",
    "\n",
    "# Hook in your own deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8343904257366469\n",
      "0.8343904\n",
      "0.8343904020331632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example 2: similarity between 2 one-word documents\n",
    "doc1a = nlp(u'large')\n",
    "doc2a = nlp(u'small')\n",
    "print(doc1a.similarity(doc2a))\n",
    "\n",
    "## first Token of both documents\n",
    "print(doc1a[0].similarity(doc2a[0]))\n",
    "\n",
    "## Token vs Doc\n",
    "print(doc1a[0].similarity(doc2a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Similarity between Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.6391e-01  4.3771e-01 -2.0447e-01 -2.2889e-01 -1.4227e-01  2.7396e-01\n",
      " -1.1435e-02 -1.8578e-01  3.7361e-01  7.5339e-01 -3.0591e-01  2.3741e-02\n",
      " -7.7876e-01 -1.3802e-01  6.6992e-02 -6.4303e-02 -4.0024e-01  1.5309e+00\n",
      " -1.3897e-02 -1.5657e-01  2.5366e-01  2.1610e-01 -3.2720e-01  3.4974e-01\n",
      " -6.4845e-02 -2.9501e-01 -6.3923e-01 -6.2017e-02  2.4559e-01 -6.9334e-02\n",
      " -3.9967e-01  3.0925e-02  4.9033e-01  6.7524e-01  1.9481e-01  5.1488e-01\n",
      " -3.1149e-01 -7.9939e-02 -6.2096e-01 -5.3277e-03 -1.1264e-01  8.3528e-02\n",
      " -7.6947e-03 -1.0788e-01  1.6628e-01  4.2273e-01 -1.9009e-01 -2.9035e-01\n",
      "  4.5630e-02  1.0120e-01 -4.0855e-01 -3.5000e-01 -3.6175e-01 -4.1396e-01\n",
      "  5.9485e-01 -1.1524e+00  3.2424e-02  3.4364e-01 -1.9209e-01  4.3255e-02\n",
      "  4.9227e-02 -5.4258e-01  9.1275e-01  2.9576e-01  2.3658e-02 -6.8737e-01\n",
      " -1.9503e-01 -1.1059e-01 -2.2567e-01  2.4180e-01 -3.1230e-01  4.2700e-01\n",
      "  8.3952e-02  2.2703e-01  3.0581e-01 -1.7276e-01  3.2536e-01  5.4696e-03\n",
      " -3.2745e-01  1.9439e-01  2.2616e-01  7.4742e-02  2.2033e-01 -4.0301e-01\n",
      " -3.1594e-01 -2.8910e-02  9.7858e-01  7.1860e-01  1.4995e-01  6.3421e-02\n",
      "  2.8332e-01 -1.5231e-01  3.9330e-04  1.8076e-01 -4.0199e-01  6.0187e-02\n",
      " -2.7543e-02  1.6590e-01 -2.5774e-01  1.6150e-01  3.7247e-01 -3.8273e-01\n",
      "  2.4012e-01 -4.2617e-02 -6.6785e-01 -9.4437e-01  2.7916e-01  1.0476e-01\n",
      "  1.3952e+00 -1.4296e-01 -5.5049e-01  5.3982e-02 -7.7524e-01 -2.8255e-01\n",
      " -2.3323e-02  2.4801e-01  2.2855e-01 -3.7408e-01  7.6012e-02  2.4031e-01\n",
      "  1.0746e-01  1.2411e-01 -2.0676e-01 -2.5804e-01 -1.6791e-01  4.3499e-01\n",
      "  6.1762e-01 -2.9955e-02  1.6196e-01 -2.9001e-01 -3.1159e-01 -8.7262e-01\n",
      "  4.3167e-01 -1.5071e-01 -4.1420e-01 -5.3730e-01 -1.9910e-01  1.3270e-01\n",
      " -1.5018e-01 -4.9335e-01 -2.5127e+00  3.1660e-01  3.6396e-01 -5.9248e-02\n",
      "  3.1120e-02  4.1071e-02  1.6917e-02  5.8410e-01 -2.0201e-01  7.0238e-02\n",
      "  8.7547e-01 -2.0114e-01  5.1920e-01  2.6786e-01 -5.5643e-01 -3.1247e-01\n",
      " -3.7992e-01  4.2857e-01  4.1780e-01  3.0608e-01 -2.1657e-01  7.2464e-01\n",
      "  6.1734e-01  5.8085e-02 -6.2708e-01  5.2895e-02 -2.5628e-01 -3.2688e-01\n",
      " -6.1280e-01  6.2609e-01 -1.7965e-01  8.8925e-01  2.1963e-01 -3.4052e-03\n",
      " -7.8663e-02  3.4799e-01 -2.6062e-01  8.0410e-03  1.1721e-01 -4.5147e-01\n",
      " -1.2178e-01 -5.7030e-01  4.6602e-01  2.5059e-02  5.3986e-02 -7.6693e-01\n",
      "  1.3173e-01 -2.8776e-02 -4.1915e-01 -2.4415e-01 -4.0295e-01 -4.1520e-01\n",
      "  3.7643e-02 -1.4843e-01  2.6094e-02  1.5315e-01  3.8310e-01 -5.5825e-01\n",
      " -3.3433e-01 -2.7939e-02 -4.3712e-01 -3.1802e-01 -3.1731e-01  9.2891e-02\n",
      " -9.9397e-02 -1.8846e-01  5.2270e-02  2.9061e-01  1.0639e+00  9.9584e-02\n",
      " -5.6775e-01  2.9446e-01  3.7797e-01 -2.1905e-01 -5.2616e-01 -4.1744e-01\n",
      " -6.5951e-01 -4.0820e-01 -6.0945e-01  1.1759e-02 -2.9122e-01 -3.1457e-01\n",
      "  5.7076e-02  4.1503e-01  3.7345e-01 -4.7119e-02 -7.1996e-02  1.4587e-01\n",
      " -3.0763e-01  1.0759e-01 -5.9447e-01 -4.0205e-01  3.0677e-01 -1.9891e-01\n",
      " -7.0775e-01 -1.1513e-01  3.0866e-01 -6.9235e-01  2.1219e-01  1.0554e-01\n",
      "  2.2617e-01 -2.6145e-01 -3.9298e-01 -2.3585e-01  3.0795e-02 -1.0193e-01\n",
      "  3.2070e-01  3.0505e-01 -5.3470e-01 -7.9272e-02 -1.6817e-01 -2.2115e-01\n",
      " -3.5143e-01 -9.2376e-02  1.4686e-01 -1.9859e-01  2.0460e-01  2.0276e-01\n",
      "  3.6144e-01 -3.5867e-01  4.0095e-01  6.3686e-02 -1.2763e-01 -1.6226e-01\n",
      " -3.1763e-01 -5.8732e-01 -5.4009e-01 -4.9035e-01 -4.6035e-01 -1.9794e-01\n",
      " -2.5209e-01  2.5706e-01  4.0110e-01  5.2830e-02 -3.2079e-01  3.9563e-01\n",
      " -4.4512e-01 -9.1862e-02 -1.9243e-01  1.5397e-01 -2.8923e-01  6.0561e-01\n",
      "  5.8133e-01  3.2268e-01  6.3892e-02  8.5438e-02  1.4956e-01  3.8134e-01\n",
      " -1.1820e-01 -2.3951e-01 -6.7731e-01  2.8090e-01 -5.1770e-01 -4.1098e-01\n",
      " -4.1292e-01 -6.7856e-02 -3.3721e-02 -7.2958e-01 -4.7891e-01  7.2956e-01]\n"
     ]
    }
   ],
   "source": [
    "# similarity on the basis of tokens\n",
    "\n",
    "doc = nlp(u\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "assert apple.similarity(banana) > pasta.similarity(hippo)\n",
    "assert apple.has_vector\n",
    "\n",
    "print(apple.vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Similarity between different object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32531983166759537\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199092090831612\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy with Russian language\n",
    "\n",
    "### taken from: https://github.com/buriy/spacy-ru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2==0.8 in /home/wohlg/.local/lib/python3.7/site-packages (0.8)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /home/wohlg/.local/lib/python3.7/site-packages (from pymorphy2==0.8) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /home/wohlg/.local/lib/python3.7/site-packages (from pymorphy2==0.8) (2.4.393442.3710985)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/wohlg/.local/lib/python3.7/site-packages (from pymorphy2==0.8) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pymorphy2==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘ru2’: File exists\n",
      "fatal: destination path 'spacy-ru' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# 1.) go to the folder where you the ipynb is\n",
    "!mkdir ru2\n",
    "!git clone -b v2.1 https://github.com/buriy/spacy-ru.git\n",
    "!cp -r ./spacy-ru/ru2/. ru2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemma \"привет\" from text \"Привет\"', 'lemma \"мир\" from text \"Миру\"', 'lemma \"!\" from text \"!\"']\n",
      "['lemma \"как\" from text \"Как\"', 'lemma \"твой\" from text \"твои\"', 'lemma \"дело\" from text \"дела\"', 'lemma \"?\" from text \"?\"']\n",
      "['lemma \"сегодня\" from text \"Сегодня\"', 'lemma \"неплохой\" from text \"неплохая\"', 'lemma \"погода\" from text \"погода\"', 'lemma \".\" from text \".\"']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sample_sentences = \"Привет Миру! Как твои дела? Сегодня неплохая погода.\"\n",
    "if __name__ == '__main__':\n",
    "    nlp = spacy.load('ru2')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'), first=True)\n",
    "    doc = nlp(sample_sentences)\n",
    "    for s in doc.sents:\n",
    "        print(list(['lemma \"{}\" from text \"{}\"'.format(t.lemma_, t.text) for t in s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos \"NOUN\" from text \"Привет\"', 'pos \"PROPN\" from text \"Миру\"', 'pos \"PUNCT\" from text \"!\"']\n",
      "['pos \"ROOT\" from text \"Привет\"', 'pos \"appos\" from text \"Миру\"', 'pos \"punct\" from text \"!\"']\n",
      "['pos \"ADV\" from text \"Как\"', 'pos \"DET\" from text \"твои\"', 'pos \"NOUN\" from text \"дела\"', 'pos \"PUNCT\" from text \"?\"']\n",
      "['pos \"mark\" from text \"Как\"', 'pos \"det\" from text \"твои\"', 'pos \"ROOT\" from text \"дела\"', 'pos \"punct\" from text \"?\"']\n",
      "['pos \"ADV\" from text \"Сегодня\"', 'pos \"ADJ\" from text \"неплохая\"', 'pos \"NOUN\" from text \"погода\"', 'pos \"PUNCT\" from text \".\"']\n",
      "['pos \"advmod\" from text \"Сегодня\"', 'pos \"amod\" from text \"неплохая\"', 'pos \"ROOT\" from text \"погода\"', 'pos \"punct\" from text \".\"']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sample_sentences)\n",
    "for s in doc.sents:\n",
    "    print(list(['pos \"{}\" from text \"{}\"'.format(t.pos_, t.text) for t in s]))\n",
    "    print(list(['pos \"{}\" from text \"{}\"'.format(t.dep_, t.text) for t in s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Миру,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Миру! Как'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1:4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"I love coffee.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.vocab.Vocab object at 0x7f83e7f819e0>\n",
      "Vocab size: 1340242\n",
      "hash value: 3197928453018144401\n",
      "string value: coffee\n"
     ]
    }
   ],
   "source": [
    "print(doc.vocab)\n",
    "print(\"Vocab size:\", len(doc.vocab))\n",
    "print('hash value:', nlp.vocab.strings['coffee'])\n",
    "print('string value:', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "# lexeme is a vocab entry\n",
    "lexeme = nlp.vocab['coffee']\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love coffee\n",
      "\n",
      "love coffee\n",
      "part 2\n"
     ]
    }
   ],
   "source": [
    "# spans are slices of the doc object\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "span = Span(doc, 1, 3)\n",
    "print(span.text)\n",
    "print(span.label_)\n",
    "\n",
    "span = Span(doc, 1, 3, label=\"part 2\")\n",
    "print(span.text)\n",
    "print(span.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5/6: spaCy rule-based matching\n",
    "\n",
    "Here we can define simple patterns, which we can match not only to the text, but also to the annotations (eg. the POS tags).\n",
    "\n",
    "This can be use for information extraction tasks.\n",
    "\n",
    "See: https://course.spacy.io/chapter1, item 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](screen1.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea\n",
    "* define patterns to match in text \n",
    "* the idea is similar to regular expressions, but the patterns are simpler, and they can match not only the text but also POS-tags, etc.\n",
    "* **in many real-world scenarios (where you don't have a large amount of training data) to train a ML model, these rule-based approaches still are applied**\n",
    "\n",
    "### Simple patterns\n",
    "* Patterns are lists of dictionaries, and each dictionary describes *one token and its attributes*. Patterns can be added to the matcher using the matcher dot add method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9528407286733565721, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9528407286733565721, 1, 3)]\n",
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "print(matches)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matches(pattern_name, pattern, doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(pattern_name, None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    # Iterate over the matches\n",
    "    print(\"Matches found:\")\n",
    "    for match_id, start, end in matches:\n",
    "        # Get the matched span\n",
    "        matched_span = doc[start:end]\n",
    "        print(matched_span.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found:\n",
      "2018 FIFA World Cup:\n",
      "2014 fifa world cup.\n"
     ]
    }
   ],
   "source": [
    "# a pattern spanning 5 tokens\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "doc = nlp(\"2018 FIFA World Cup: France won! 2014 fifa world cup. The 2010 fifa world cup was in ...\")\n",
    "\n",
    "print_matches('Fifa pattern', pattern, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "\n",
    "### What does this next pattern do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found:\n",
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "# multiple conditions for one token\n",
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "print_matches('pattern2', pattern, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex patterns:\n",
    "* OP to say have often a patterns can match: \"+\" will match one or more times, \"?\" zero or one times, \"!\" match zero times (negation), \"*\" match 0-n times\n",
    "* LEMMA, POS, LOWER, TEXT, OP, ENT_TYPE (https://spacy.io/usage/rule-based-matching#adding-patterns-attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches found:\n",
      "I bought a smartphone\n",
      "'m buying apps\n"
     ]
    }
   ],
   "source": [
    "# match on token 2 is optional\n",
    "pattern = [\n",
    "    {'ENT_TYPE': 'PERSON', 'OP': '!'},  # optional: match 0 or 1 times\n",
    "    {'LEMMA': 'buy'},\n",
    "    {'POS': 'DET', 'OP': '?'},  # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps. Arnold bought a smartphone. \")\n",
    "\n",
    "print_matches(\"buy a pattern\", pattern, doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: combine matcher and statistical model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### online demo to create patterns visually and test them:\n",
    "https://explosion.ai/demos/matcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">\n",
    "    \n",
    "## Exercise 5: simple matching\n",
    "Image you want to find (for example in all of Wikipedia), which Persons and Organizations appear in sentences together with \"Albert Einstein\".\n",
    "* Write the patterns\n",
    "* use the pattern to collect the persons and organistations\n",
    "* Test on a few example sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopped here Unit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhraseMatcher: fast matching of phrases\n",
    "* PhraseMatcher like regular expressions or keyword search – but with access to the tokens!\n",
    "* Takes Doc object as patterns\n",
    "* More efficient and faster than the Matcher\n",
    "* Great for matching large word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy processing pipelines\n",
    "\n",
    "* We already know the basic pipeline in the nlp() function: tokenization, lemmatization, POS-tagging, dependency parsing, ...\n",
    "* We exchange compents or add custom ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pipe.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pipe2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pipe3.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# show current pipeline\n",
    "print(nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f6bf63b2f90>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f6bf64ee980>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f6bf64eede0>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom pipeline components\n",
    "* will be executed automatically when nlp() is used\n",
    "* you can updated built-in attributes like doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'compute_length', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "def compute_length(doc):\n",
    "    # Do something to the doc here\n",
    "    print('Doc length:', len(doc))\n",
    "    return doc\n",
    "\n",
    "if nlp.has_pipe(\"compute_length\"):\n",
    "    nlp.remove_pipe(\"compute_length\")\n",
    "    \n",
    "nlp.add_pipe(compute_length, after='tagger')\n",
    "# nlp.add_pipe(component, last=True)\n",
    "# nlp.add_pipe(component, first=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=\"red\">\n",
    "    \n",
    "## Exercise 6: simple pipeline extentions\n",
    "* write a function that checks if the document is in English language (doc.lang_)\n",
    "* if the document is not in English, print \"wrong language, use another pipeline\"\n",
    "* add another function to the pipeline, which check is if the document contains a phrase of two consecutive nouns. If such phrases exist, they are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test document\n",
      "document retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'POS': 'NOUN'}, {'POS': 'NOUN'}])\n",
    "\n",
    "\n",
    "def check_english(doc):\n",
    "\n",
    "    if not doc.lang_ == \"en\":\n",
    "        print(\"Wrong Language!\")\n",
    "    return doc\n",
    "\n",
    "def check_dog(doc):\n",
    "\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        print(doc[start:end])\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(check_english, after='ner')\n",
    "nlp.add_pipe(check_dog, after='check_english')\n",
    "\n",
    "doc = nlp(\"This is a test document retriever.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting custom attributes, properties and methods\n",
    "\n",
    "### Attribute extensions\n",
    "* for the Doc, Span and Token objects\n",
    "* set attributes for each of these objects\n",
    "* for example: doc.\\_.title = \"bla\"  ... the ._. is used to distinguish custom and built-in attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'set_color', 'ner', 'check_english', 'check_dog']\n"
     ]
    }
   ],
   "source": [
    "def set_color(doc):    \n",
    "    doc._.title = 'My document'\n",
    "    doc[0]._.is_color = False # token\n",
    "    doc[0:2]._.has_color = True # span\n",
    "    return doc\n",
    "\n",
    "    \n",
    "nlp.add_pipe(set_color, after='parser')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n",
      "My document\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.title)\n",
    "# print(doc._.is_color) # throws an exception!\n",
    "print(doc[0]._.is_color)\n",
    "print(doc[1]._.is_color)\n",
    "print(doc[0:2]._.has_color)\n",
    "print(doc[0:3]._.has_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test document\n",
      "document retriever\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "  \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'POS': 'NOUN'}, {'POS': 'NOUN'}])\n",
    "\n",
    "#  Span.set_extension('noun_phrase', default=False)\n",
    "\n",
    "\n",
    "\n",
    "def check_english(doc):\n",
    "\n",
    "    if not doc.lang_ == \"en\":\n",
    "        print(\"Wrong Language!\")\n",
    "    return doc\n",
    "\n",
    "def check_dog(doc):\n",
    "\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        print(doc[start:end])\n",
    "        doc[start:end]._.noun_phrase = True\n",
    "\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(check_english, after='ner')\n",
    "nlp.add_pipe(check_dog, after='check_english')\n",
    "\n",
    "doc = nlp(\"This is a test document retriever.\")\n",
    "print(doc[3:5]._.noun_phrase)\n",
    "print(doc[0:2]._.noun_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=\"red\">\n",
    "\n",
    "## Exercise 7: simple attribute\n",
    "* extend the solution of exercise 6, add a Span attribute \"noun_phrase\" to the respective span\n",
    "* Test with a few example docs if it works "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property extensions\n",
    "* getter (and optional: setting) functions are called \n",
    "* especially if you want to set attributes on a Span you should use this (getter functions)!\n",
    "* As I (Gerhard) understand it, these functions are not called in the nlp() function, but on-demand when the property is accessed\n",
    "* The function does not modify the doc object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n",
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension('is_color1', getter=get_is_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color1, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n",
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension('has_color1', getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color1, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color1, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally: Method extensions\n",
    "* Assign a function that becomes available as an object method\n",
    "* Lets you pass arguments to the extension function\n",
    "* Again (as I understand), the function is called only on-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 5\n",
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy https://course.spacy.io/chapter4 discusses how to change the machine learning models \n",
    "* it's a very simple process\n",
    "* but we don't have time to cover it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of spaCy notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
